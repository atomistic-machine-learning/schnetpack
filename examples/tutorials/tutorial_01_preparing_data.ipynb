{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and loading your data\n",
    "This tutorial introduces how SchNetPack stores and loads data.\n",
    "Before we can start training neural networks with SchNetPack, we need to prepare our data.\n",
    "This is because SchNetPack has to stream the reference data from disk during training in order to be able to handle large datasets.\n",
    "Therefore, it is crucial to use data format that allows for fast random read access.\n",
    "We found that the [ASE database format](https://wiki.fysik.dtu.dk/ase/ase/db/db.html) fulfills this criterion perfectly.\n",
    "To further improve the performance, we internally encode properties in binary.\n",
    "However, as long as you only access the ASE database via the provided SchNetPack `ASEAtomsData` class, you don't have to worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schnetpack.data import ASEAtomsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined datasets\n",
    "SchNetPack supports several benchmark datasets that can be used without preparation.\n",
    "Each one can be accessed using a corresponding class that inherits from `AtomsDataModule` (a specialized PyTorchLightning `DataModule`), which supports automatic download, conversion and partitioning. Here, we show how to use these data sets at the example of the QM9 benchmark.\n",
    "\n",
    "First, we have to import the dataset class and instantiate it. This will automatically download the data to the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schnetpack.datasets import QM9\n",
    "from schnetpack.transform import ASENeighborList\n",
    "\n",
    "qm9data = QM9(\n",
    "    './qm9.db', \n",
    "    batch_size=10,\n",
    "    num_train=110000,\n",
    "    num_val=10000,\n",
    "    transforms=[ASENeighborList(cutoff=5.)]\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbors are collected using neighborlists that can be passed to the `AtomsDataModule` as a preprocessing transform. These are applied to the molecules before they are batched in the data loader. We supply different environment providers using a cutoff (e.g., `AseEnvironmentProvider`, `TorchEnvironmentProvider`) that are able to handle larger molecules and periodic boundary conditions.\n",
    "\n",
    "Let's have a closer look at this dataset.\n",
    "We can find out how large it is and which properties it supports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reference calculations: 133885\n",
      "Number of train data: 110000\n",
      "Number of validation data: 10000\n",
      "Number of test data: 13885\n",
      "Available properties:\n",
      "- rotational_constant_A\n",
      "- rotational_constant_B\n",
      "- rotational_constant_C\n",
      "- dipole_moment\n",
      "- isotropic_polarizability\n",
      "- homo\n",
      "- lumo\n",
      "- gap\n",
      "- electronic_spatial_extent\n",
      "- zpve\n",
      "- energy_U0\n",
      "- energy_U\n",
      "- enthalpy_H\n",
      "- free_energy\n",
      "- heat_capacity\n"
     ]
    }
   ],
   "source": [
    "print('Number of reference calculations:', len(qm9data.dataset))\n",
    "print('Number of train data:', len(qm9data.train_dataset))\n",
    "print('Number of validation data:', len(qm9data.val_dataset))\n",
    "print('Number of test data:', len(qm9data.test_dataset))\n",
    "print('Available properties:')\n",
    "\n",
    "for p in qm9data.dataset.available_properties:\n",
    "    print('-', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load data points  using zero-base indexing. The result is a dictionary containing the geometry and properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties:\n",
      "- _idx : torch.Size([1])\n",
      "- rotational_constant_A : torch.Size([1])\n",
      "- rotational_constant_B : torch.Size([1])\n",
      "- rotational_constant_C : torch.Size([1])\n",
      "- dipole_moment : torch.Size([1])\n",
      "- isotropic_polarizability : torch.Size([1])\n",
      "- homo : torch.Size([1])\n",
      "- lumo : torch.Size([1])\n",
      "- gap : torch.Size([1])\n",
      "- electronic_spatial_extent : torch.Size([1])\n",
      "- zpve : torch.Size([1])\n",
      "- energy_U0 : torch.Size([1])\n",
      "- energy_U : torch.Size([1])\n",
      "- enthalpy_H : torch.Size([1])\n",
      "- free_energy : torch.Size([1])\n",
      "- heat_capacity : torch.Size([1])\n",
      "- _n_atoms : torch.Size([1])\n",
      "- _atomic_numbers : torch.Size([5])\n",
      "- _positions : torch.Size([5, 3])\n",
      "- _cell : torch.Size([1, 3, 3])\n",
      "- _pbc : torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "example = qm9data.dataset[0]\n",
    "print('Properties:')\n",
    "\n",
    "for k, v in example.items():\n",
    "    print('-', k, ':', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all available properties have been loaded as torch tensors with the given shapes. Keys with an underscore indicate that these names are reserved for internal use. This includes the geometry (`_n_atoms`, `_atomic_numbers`, `_positions`), the index within the dataset (`_idx`) as well as information about neighboring atoms and periodic boundary conditions (`_cell`, `_pbc`). \n",
    "\n",
    "\n",
    "We can iterate the dataset partitions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_idx', 'rotational_constant_A', 'rotational_constant_B', 'rotational_constant_C', 'dipole_moment', 'isotropic_polarizability', 'homo', 'lumo', 'gap', 'electronic_spatial_extent', 'zpve', 'energy_U0', 'energy_U', 'enthalpy_H', 'free_energy', 'heat_capacity', '_n_atoms', '_atomic_numbers', '_positions', '_cell', '_pbc', '_idx_i_local', '_idx_j_local', '_offsets', '_idx_m', '_idx_i', '_idx_j'])\n"
     ]
    }
   ],
   "source": [
    "for batch in qm9data.val_dataloader():\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that additional keys have been added by the neighborlist transform defined above. These are the relative positions (`_Rij`) and neighbor indices (`_idx_i`, `_idx_j`). Since differrent systems can have different numbers of atoms, we don't use separate dimensions for systems and atoms (i.e. shape [n_systems, n_atoms, ...]), but store the atoms of all systems in a single dimension (i.e. shape [n_all_atoms, ...]). Therefore, we additionally need to store the indices of the corresponding system for each atom in a batch (`idx_m`). This avoids the padding and masking that was required in previous versions of SchNetPack. The indices look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System index: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n",
      "Center atom index: tensor([  0,   0,   0,  ..., 155, 155, 155])\n",
      "Neighbor atom index: tensor([  1,  14,  12,  ..., 141, 146, 154])\n"
     ]
    }
   ],
   "source": [
    "print('System index:', batch['_idx_m'])\n",
    "print('Center atom index:', batch['_idx_i'])\n",
    "print('Neighbor atom index:', batch['_idx_j'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All property names are pre-defined as class-variable for convenient access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total energy at 0K: tensor([-450.2210, -437.8808, -333.4494, -403.1605, -387.1264, -472.7010,\n",
      "        -385.8485, -470.0007, -434.0736, -453.9216], dtype=torch.float64)\n",
      "HOMO: tensor([-0.2451, -0.2660, -0.2690, -0.2507, -0.2318, -0.2676, -0.2343, -0.2327,\n",
      "        -0.2768, -0.2193], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print('Total energy at 0K:', batch[QM9.U0])\n",
    "print('HOMO:', batch[QM9.homo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your own data\n",
    "In the following we will create an ASE database from our own data.\n",
    "For this tutorial, we will use a dataset containing a molecular dynamics (MD) trajectory of ethanol, which can be downloaded [here](http://quantum-machine.org/gdml/data/xyz/ethanol_dft.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./uracil_dft.npz'):\n",
    "    !wget http://quantum-machine.org/gdml/data/npz/uracil_dft.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is in Numpy format. \n",
    "In the following, we show how this data can be parsed and converted for use in SchNetPack, so that you apply this to any other data format.\n",
    "\n",
    "First, we need to parse our data. For this we use the IO functionality supplied by ASE.\n",
    "In order to create a SchNetPack DB, we require a **list of ASE `Atoms` objects** as well as a corresponding **list of dictionaries** `[{property_name1: property1_molecule1}, {property_name1: property1_molecule2}, ...]` containing the mapping from property names to values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties: {'energy': array([-260120.11049893]), 'forces': array([[ 8.58762909e-01,  7.93783439e+00, -6.10513446e-01],\n",
      "       [-1.59587602e+01, -1.38921103e+01,  1.46274031e+00],\n",
      "       [ 2.38826967e+01,  6.32292299e+01, -5.38152591e+00],\n",
      "       [-2.39499309e+01, -5.43241035e+00,  1.04116149e+00],\n",
      "       [ 6.26392709e+01, -6.78667364e+01,  3.52832279e+00],\n",
      "       [-2.38302859e+01,  8.54756612e+00, -3.88276505e-02],\n",
      "       [ 2.46907632e+00,  3.17072923e-01, -9.76762146e-02],\n",
      "       [ 1.47304240e+01,  1.43791066e+01, -1.46191820e+00],\n",
      "       [-6.34016421e+00,  2.75814146e+00, -5.13516521e-02],\n",
      "       [-1.13403416e+01, -1.98585305e+01,  1.77899100e+00],\n",
      "       [-1.58345920e+01,  1.17016462e+01, -4.86097667e-01],\n",
      "       [-7.32615606e+00, -1.82081008e+00,  3.16695146e-01]])}\n"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "import numpy as np\n",
    "\n",
    "# load atoms from npz file. Here, we only parse the first 10 molecules\n",
    "data = np.load('./uracil_dft.npz')\n",
    "\n",
    "numbers = data[\"z\"]\n",
    "atoms_list = []\n",
    "property_list = []\n",
    "for positions, energies, forces in zip(data[\"R\"], data[\"E\"], data[\"F\"]):\n",
    "    ats = Atoms(positions=positions, numbers=numbers)\n",
    "    properties = {'energy': energies, 'forces': forces}\n",
    "    property_list.append(properties)\n",
    "    atoms_list.append(ats)\n",
    "    \n",
    "print('Properties:', property_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data in this format, it is straightforward to create a new SchNetPack DB and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm './new_dataset.db'\n",
    "new_dataset = ASEAtomsData.create(\n",
    "    './new_dataset.db', \n",
    "    distance_unit='Ang',\n",
    "    property_unit_dict={'energy':'kcal/mol', 'forces':'kcal/mol/Ang'}\n",
    ")\n",
    "new_dataset.add_systems(property_list, atoms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have more diverse data with systems of different composition and size, you might want to add single atom reference values to your dataset as metadata. This can be used by the RemoveOffset/AddOffset transforms to have it removed from the target property during training, and added to the prediction in post-processing during inference. This is especially useful for extensive properties such as the energy, where the single atom energies contribute a major part to the overall value. This leads to a better initialization of the network and avoids numerical issues, when the casting to float32 for GPU training happens only after removing the offset.\n",
    "\n",
    "Here is an example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# calculate this at the same level of theory as your data\n",
    "atomref = {\n",
    "'energy': [0.0, 314.0,  ] # etc.\n",
    "}\n",
    "\n",
    "# the supplied list is ordered by atomic number, e.g.:\n",
    "atomref_hydrogen= atomref['energy'][1]\n",
    "\n",
    "# dataset = ASEAtomsData.create(\n",
    "#     './new_dataset.db',\n",
    "#     distance_unit='Ang',\n",
    "#     property_unit_dict={'energy':'kcal/mol'},\n",
    "#     atomref=atomref\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our concrete case, we only have an MD trajectory of a single system. Therefore, we don't need to specify an atomref, since removing the average energy will working as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a look at the data in the same way we did before for QM9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reference calculations: 133770\n",
      "Available properties:\n",
      "- energy\n",
      "- forces\n",
      "\n",
      "Properties of molecule with id 0:\n",
      "- _idx : torch.Size([1])\n",
      "- energy : torch.Size([1])\n",
      "- forces : torch.Size([12, 3])\n",
      "- _n_atoms : torch.Size([1])\n",
      "- _atomic_numbers : torch.Size([12])\n",
      "- _positions : torch.Size([12, 3])\n",
      "- _cell : torch.Size([1, 3, 3])\n",
      "- _pbc : torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print('Number of reference calculations:', len(new_dataset))\n",
    "print('Available properties:')\n",
    "\n",
    "for p in new_dataset.available_properties:\n",
    "    print('-', p)\n",
    "print()    \n",
    "\n",
    "example = new_dataset[0]\n",
    "print('Properties of molecule with id 0:')\n",
    "\n",
    "for k, v in example.items():\n",
    "    print('-', k, ':', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same way, we can store multiple properties, including atomic properties such as forces, or tensorial properties such as polarizability tensors.\n",
    "\n",
    "In the following tutorials, we will describe how these datasets can be used to train neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spkdev] *",
   "language": "python",
   "name": "conda-env-spkdev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
