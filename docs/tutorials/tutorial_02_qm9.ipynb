{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network on QM9\n",
    "\n",
    "This tutorial will explain how to use SchNetPack for training a model\n",
    "on the QM9 dataset and how the trained model can be used for further.\n",
    "\n",
    "First, we import the necessary modules and create a new directory for the data and our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import schnetpack as spk\n",
    "\n",
    "qm9tut = './qm9tut'\n",
    "if not os.path.exists('qm9tut'):\n",
    "    os.makedirs(qm9tut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "As explained in the [previous tutorial](tutorial_01_preparing_data.ipynb), datasets in SchNetPack are loaded with the `AtomsLoader` class or one of the sub-classes that are specialized for common benchmark datasets. \n",
    "The `QM9` dataset class will download and convert the data. We will only use the inner energy at 0K `U0`, so all other properties do not need to be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kschuett/code/spkdev/src/schnetpack/transform/atomistic.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.atomref.copy_(torch.tensor(atrefs[self._property]))\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from schnetpack.datasets import QM9\n",
    "\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "%rm split.npz\n",
    "\n",
    "qm9data = QM9(\n",
    "    './qm9.db', \n",
    "    batch_size=100,\n",
    "    num_train=10000,\n",
    "    num_val=1000,\n",
    "    transforms=[\n",
    "        trn.TorchNeighborList(cutoff=5.),\n",
    "        trn.RemoveOffsets(QM9.U0, remove_mean=True, remove_atomrefs=True)\n",
    "    ],\n",
    "    property_units={QM9.U0: 'eV'}\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is downloaded and partitioned automatically. PyTorch `DataLoader`s can be obtained using `qm9data.train_dataloader()`, `qm9data.val_dataloader()` and `qm9data.test_dataloader()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset offsets\n",
    "\n",
    "Before building the model, we remove offsets from the energy for good initial conditions. We will get this from the training dataset. Above this is done automatically by the `RemoveOffsets` transform. \n",
    "\n",
    "In the following we show what happens under the hood.\n",
    "For QM9, we also have single-atom reference values stored in the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U0 of hyrogen: -13.613121032714844 eV\n",
      "U0 of carbon: -1029.863037109375 eV\n",
      "U0 of oxygen: -2042.611083984375 eV\n"
     ]
    }
   ],
   "source": [
    "atomrefs = qm9data.train_dataset.atomrefs\n",
    "print('U0 of hyrogen:', atomrefs[QM9.U0][1].item(), 'eV')\n",
    "print('U0 of carbon:', atomrefs[QM9.U0][6].item(), 'eV')\n",
    "print('U0 of oxygen:', atomrefs[QM9.U0][8].item(), 'eV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be used together with the mean and standard deviation of the energy per atom to initialize the model with a good guess of the energy of a molecule. When calculating these statistics, we pass the atomref to take into account, that the model will add these atomrefs to the predicted energy later, so that this part of the energy does not have to be considered in the statistics, i.e.\n",
    "\\begin{equation}\n",
    "\\mu_{U_0} = \\frac{1}{n_\\text{train}} \\sum_{n=1}^{n_\\text{train}} \\left( U_{0,n} - \\sum_{i=1}^{n_{\\text{atoms},n}} U_{0,Z_{n,i}} \\right)\n",
    "\\end{equation}\n",
    "for the mean and analog for the standard deviation. In this case, this corresponds to the mean and std. dev of the *atomization energy* per atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:10<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean atomization energy / atom: -4.245539158975564\n",
      "Std. dev. atomization energy / atom: 0.1912277651100761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "means, stddevs = qm9data.get_stats(\n",
    "    QM9.U0, divide_by_atoms=True, remove_atomref=True\n",
    ")\n",
    "print('Mean atomization energy / atom:', means.item())\n",
    "print('Std. dev. atomization energy / atom:', stddevs.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "The next step is to build the neural network model.\n",
    "This consists of 2 parts:\n",
    "\n",
    "1. The representation part which either constructs atom-wise features, e.g. with SchNet, or build a fixed descriptor such as atom-centered symmetry functions.\n",
    "2. One or more output modules for property prediction.\n",
    "\n",
    "We will use a `SchNet` module with 3 interaction layers, a 5 Angstrom cosine cutoff with pairwise distances expanded on 20 Gaussians and 50 atomwise features and convolution filters here, since we only have a few training examples (click [here](../modules/representation.rst#module-schnetpack.representation.schnet) for details on SchNet). Then, we will use one `Atomwise` modules to predict the energy, which takes mean and standard deviation per atom of the property for initialization. Both modules are then combined to an `AtomisticModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'datamodule', 'schedule', and 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-54aa7c8cb39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moutput_U0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAtomwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSinglePropertyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_U0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'datamodule', 'schedule', and 'optimizer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cutoff = 5.\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)\n",
    "schnet = spk.representation.SchNet(\n",
    "    n_atom_basis=30, n_interactions=3,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff)\n",
    ")\n",
    "\n",
    "output_U0 = spk.atomistic.Atomwise(n_in=30)\n",
    "model = spk.model.SinglePropertyModel(\n",
    "    datamodule=qm9, \n",
    "    representation=schnet, \n",
    "    output=output_U0, \n",
    "    schedule=spk.,\n",
    "    optimizer=torch.optim.AdamW(lr=5e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model, we will use the `Trainer` class that comes with SchNetPack.\n",
    "For this, we need to first define a loss function and select an optimizer.\n",
    "As the loss function, we will use the mean squared error of the energy\n",
    "$\\ell(E_\\text{ref}, E_\\text{pred}) = \\frac{1}{n_\\text{train}} \\sum_{n=1}^{n_\\text{train}} (E_\\text{ref} - E_\\text{pred})^2$\n",
    "\n",
    "This will be minimized using the Adam optimizer from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# loss function\n",
    "def mse_loss(batch, result):\n",
    "    diff = batch[QM9.U0]-result[QM9.U0]\n",
    "    err_sq = torch.mean(diff ** 2)\n",
    "    return err_sq\n",
    "\n",
    "# build optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give the `Trainer` hooks, that are called at certain points during the training loop.\n",
    "This is useful to customize the training process, e.g. with logging, learning rate schedules or stopping criteria.\n",
    "Here, we set up a basis logging as well as a learning rate schedule that reduces the learning rate by factor 0.8 after 5 epochs without improvement of the validation loss.\n",
    "\n",
    "The logger takes a list of validation metrics that specify what is going to be stored.\n",
    "In this example, we want to log the mean absolute and root mean squared error of the $U_0$ energy prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before setting up the trainer, remove previous training checkpoints and logs\n",
    "%rm -r ./qm9tut/checkpoints\n",
    "%rm -r ./qm9tut/log.csv\n",
    "\n",
    "import schnetpack.train as trn\n",
    "\n",
    "loss = trn.build_mse_loss([QM9.U0])\n",
    "\n",
    "metrics = [spk.metrics.MeanAbsoluteError(QM9.U0)]\n",
    "hooks = [\n",
    "    trn.CSVHook(log_path=qm9tut, metrics=metrics), \n",
    "    trn.ReduceLROnPlateauHook(\n",
    "        optimizer, \n",
    "        patience=5, factor=0.8, min_lr=1e-6,\n",
    "        stop_after_min=True\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = trn.Trainer(\n",
    "    model_path=qm9tut,\n",
    "    model=model,\n",
    "    hooks=hooks,\n",
    "    loss_fn=loss,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the training for a given number of epochs. If we don't give a number, the trainer runs until a stopping criterion is met. For the purpose of this tutorial, we let it train for 200 epochs (on GPU this should take about 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # change to 'cpu' if gpu is not available\n",
    "n_epochs = 200 # takes about 10 min on a notebook GPU. reduces for playing around\n",
    "trainer.train(device=device, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call this method several times to continue training. For the training to run until convergence, remove the n_epochs argument or set it to a very large number.\n",
    "\n",
    "Let us finally have a look at the CSV log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation MAE: 0.14 eV = 3.2 kcal/mol\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ase.units import kcal, mol\n",
    "\n",
    "results = np.loadtxt(os.path.join(qm9tut, 'log.csv'), skiprows=1, delimiter=',')\n",
    "\n",
    "time = results[:,0]-results[0,0]\n",
    "learning_rate = results[:,1]\n",
    "train_loss = results[:,2]\n",
    "val_loss = results[:,3]\n",
    "val_mae = results[:,4]\n",
    "\n",
    "print('Final validation MAE:', np.round(val_mae[-1], 2), 'eV =',\n",
    "      np.round(val_mae[-1] / (kcal/mol), 2), 'kcal/mol')\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(time, val_loss, label='Validation')\n",
    "plt.plot(time, train_loss, label='Train')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss [eV]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(time, val_mae)\n",
    "plt.ylabel('mean abs. error [eV]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the prediction can be improved by letting the training run longer, increasing the patience, the number of neurons and interactions or using regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "\n",
    "Having trained a model for QM9, we are going to use it to obtain some predictions.\n",
    "First, we need to load the model. The `Trainer` stores the best model in the model directory which can be loaded using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "best_model = torch.load(os.path.join(qm9tut, 'best_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate it on the test data, we create a data loader for the test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328\n",
      "Progress: 87.73%\r"
     ]
    }
   ],
   "source": [
    "test_loader = spk.AtomsLoader(test, batch_size=100)\n",
    "\n",
    "err = 0\n",
    "print(len(test_loader))\n",
    "for count, batch in enumerate(test_loader):    \n",
    "    # move batch to GPU, if necessary\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # apply model\n",
    "    pred = best_model(batch)\n",
    "    \n",
    "    # calculate absolute error\n",
    "    tmp = torch.sum(torch.abs(pred[QM9.U0]-batch[QM9.U0]))\n",
    "    tmp = tmp.detach().cpu().numpy() # detach from graph & convert to numpy\n",
    "    err += tmp\n",
    "    \n",
    "    # log progress\n",
    "    percent = '{:3.2f}'.format(count/len(test_loader)*100)\n",
    "    print('Progress:', percent+'%'+' '*(5-len(percent)), end=\"\\r\")\n",
    "    \n",
    "err /= len(test)\n",
    "print('Test MAE', np.round(err, 2), 'eV =',\n",
    "      np.round(err / (kcal/mol), 2), 'kcal/mol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is not already in SchNetPack format, a more convenient way is to use ASE atoms objects with the provided `AtomsConverter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = spk.data.AtomsConverter(device=device)\n",
    "at, props = test.get_properties(idx=153)\n",
    "\n",
    "inputs = converter(at)\n",
    "print('Keys:', list(inputs.keys()))\n",
    "print('Truth:', props[QM9.U0].cpu().numpy()[0])\n",
    "\n",
    "pred = model(inputs)\n",
    "\n",
    "print('Prediction:', pred[QM9.U0].detach().cpu().numpy()[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can use the `SpkCalculator` as an interface to ASE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = spk.interfaces.SpkCalculator(model=model, device=device, energy=QM9.U0)\n",
    "at.set_calculator(calculator)\n",
    "print('Prediction:', at.get_total_energy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, we have trained and evaluated a SchNet model on a small subset of QM9.\n",
    "A full training script with is available [here](../../src/examples/qm9_tutorial.py).\n",
    "\n",
    "In the next tutorial, we will show how to learn potential energy surfaces and forces field as well as performing molecular dynamics simulations with SchNetPack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
