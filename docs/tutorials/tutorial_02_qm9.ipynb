{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network on QM9\n",
    "\n",
    "This tutorial will explain how to use SchNetPack for training a model\n",
    "on the QM9 dataset and how the trained model can be used for further.\n",
    "\n",
    "First, we import the necessary modules and create a new directory for the data and our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import schnetpack as spk\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "qm9tut = './qm9tut'\n",
    "if not os.path.exists('qm9tut'):\n",
    "    os.makedirs(qm9tut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "As explained in the [previous tutorial](tutorial_01_preparing_data.ipynb), datasets in SchNetPack are loaded with the `AtomsLoader` class or one of the sub-classes that are specialized for common benchmark datasets. \n",
    "The `QM9` dataset class will download and convert the data. We will only use the inner energy at 0K `U0`, so all other properties do not need to be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'split.npz': No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "%rm split.npz\n",
    "\n",
    "qm9data = QM9(\n",
    "    './qm9.db', \n",
    "    batch_size=100,\n",
    "    num_train=1000,\n",
    "    num_val=1000,\n",
    "    transforms=[\n",
    "        trn.TorchNeighborList(cutoff=5.),\n",
    "        trn.RemoveOffsets(QM9.U0, remove_mean=True, remove_atomrefs=True),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    property_units={QM9.U0: 'eV'},\n",
    "    num_workers=1,\n",
    "    split_file=os.path.join(qm9tut, \"split.npz\")\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is downloaded and partitioned automatically. PyTorch `DataLoader`s can be obtained using `qm9data.train_dataloader()`, `qm9data.val_dataloader()` and `qm9data.test_dataloader()`.\n",
    "\n",
    "Before building the model, we remove offsets from the energy for good initial conditions. We will get this from the training dataset. Above this is done automatically by the `RemoveOffsets` transform.\n",
    "In the following we show what happens under the hood.\n",
    "For QM9, we also have single-atom reference values stored in the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U0 of hyrogen: -13.613121032714844 eV\n",
      "U0 of carbon: -1029.863037109375 eV\n",
      "U0 of oxygen: -2042.611083984375 eV\n"
     ]
    }
   ],
   "source": [
    "atomrefs = qm9data.train_dataset.atomrefs\n",
    "print('U0 of hyrogen:', atomrefs[QM9.U0][1].item(), 'eV')\n",
    "print('U0 of carbon:', atomrefs[QM9.U0][6].item(), 'eV')\n",
    "print('U0 of oxygen:', atomrefs[QM9.U0][8].item(), 'eV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be used together with the mean and standard deviation of the energy per atom to initialize the model with a good guess of the energy of a molecule. When calculating these statistics, we pass the atomref to take into account, that the model will add these atomrefs to the predicted energy later, so that this part of the energy does not have to be considered in the statistics, i.e.\n",
    "\\begin{equation}\n",
    "\\mu_{U_0} = \\frac{1}{n_\\text{train}} \\sum_{n=1}^{n_\\text{train}} \\left( U_{0,n} - \\sum_{i=1}^{n_{\\text{atoms},n}} U_{0,Z_{n,i}} \\right)\n",
    "\\end{equation}\n",
    "for the mean and analog for the standard deviation. In this case, this corresponds to the mean and std. dev of the *atomization energy* per atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean atomization energy / atom: -4.248723880126298\n",
      "Std. dev. atomization energy / atom: 0.19338399600016018\n"
     ]
    }
   ],
   "source": [
    "means, stddevs = qm9data.get_stats(\n",
    "    QM9.U0, divide_by_atoms=True, remove_atomref=True\n",
    ")\n",
    "print('Mean atomization energy / atom:', means.item())\n",
    "print('Std. dev. atomization energy / atom:', stddevs.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "Next, we need to build the model and define how it should be trained\n",
    "\n",
    "In SchNetPack, a model usually consists of two parts:\n",
    "1. The representation which either constructs atom-wise features, e.g. with SchNet or PaiNN.\n",
    "2. One or more output modules for property prediction.\n",
    "\n",
    "Here, we use the `SchNet` representation with 3 interaction layers, a 5 Angstrom cosine cutoff with pairwise distances\n",
    "expanded on 20 Gaussians and 50 atomwise features and convolution filters here, since we only have a few\n",
    "training examples. Then, we use an `Atomwise` module to predict the inner energy $U_0$ by summing over atom-wise\n",
    "energy contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5.\n",
    "n_atom_basis = 30\n",
    "\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)\n",
    "schnet = spk.representation.SchNet(\n",
    "    n_atom_basis=n_atom_basis, n_interactions=3,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff)\n",
    ")\n",
    "\n",
    "pred_U0 = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key=QM9.U0)\n",
    "output_U0 = spk.atomistic.ModelOutput(\n",
    "    name=QM9.U0,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1.,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output model stores the prediction in the result dictionary under the `output_key`, which is mapped to a\n",
    "a target with loss function using the `ModelOutput`.\n",
    "By default, the target is assumed to have the same name as the output. Otherwise, a different `target_name`\n",
    "has to be provided.\n",
    "Here, we already gave the output the same name as the target in the dataset (`QM9.U0`).\n",
    "In case of multiple outputs, the full loss is a weighted sum of all output losses.\n",
    "Therefore, it is possible to provide a `loss_weight`, which we here just set to 1.\n",
    "\n",
    "All components defined above are then passed to `AtomisticModel`, which is a sublass of\n",
    "[`LightningModule`](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "This defines the model and training process and can then be passed to the PyTorch Lightning `Trainer`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = spk.atomistic.AtomisticModel(\n",
    "    representation=schnet,\n",
    "    output_modules=[pred_U0],\n",
    "    outputs=[output_U0],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\":5e-4},\n",
    "    postprocess=[trn.CastTo64(), trn.AddOffsets(QM9.U0, add_mean=True, add_atomrefs=True)]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last part here is a list of postprocessors that will only be used if `model.inference_mode=True` is set.\n",
    "This will not be used in training or validation, but only for predictions.\n",
    "Here, this is used to deal with numerical accuracy and normalization of model outputs:\n",
    "To make training easier, we have substracted single atom energies as well as the mean energy per atom\n",
    "in the preprocessing (see above).\n",
    "This does not matter for the loss, but for the final prediction we want to get the real energies.\n",
    "Additionally, we have removed the energy offsets *before* casting to float32 in the preprocessor.\n",
    "This avoids loss of numerical precision.\n",
    "Analog to this, we also have to first cast to float64, before re-adding the offsets in the post-processor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now, the model is ready for training. Since we already defined all necessary components, wthe only thing left to do is\n",
    "passing it to the PyTorch Lightning `Trainer` together with the data module.\n",
    "\n",
    "Additionally, we can provide callback that take care of logging, checkpointing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1303: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | representation | SchNet     | 16.0 K\n",
      "1 | outputs        | ModuleList | 0     \n",
      "2 | output_modules | ModuleList | 481   \n",
      "3 | postprocessors | ModuleList | 0     \n",
      "----------------------------------------------\n",
      "16.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.4 K    Total params\n",
      "0.066     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea66b154d5044f158485eb5f896d2f85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: -1it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b4da9b34095424b9fc79837ed60fcf0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f05c4cb00ec4e27a0b6911a8d6d8e2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2955dc89f8f4c6db1d3738e60cccdc7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98c904412ced4d5bb257cdc85ae716c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir=qm9tut)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        inference_path=os.path.join(qm9tut, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=qm9tut,\n",
    "    max_epochs=3, # for testing, we restrict the number of epochs\n",
    ")\n",
    "trainer.fit(model, datamodule=qm9data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ModelCheckpoint` of SchNetPack is equivalent to that in PyTorch Lightning,\n",
    "except that we also store the best inference model. We will show how to use thi in the next section.\n",
    "\n",
    "You can have a look at the log using Tensorboard:\n",
    "```\n",
    "tensorboard --logdir=qm9tut/default\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Having trained a model for QM9, we are going to use it to obtain some predictions.\n",
    "First, we need to load the model. The `Trainer` stores the best model in the model directory which can be loaded using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "\n",
    "best_model = torch.load(os.path.join(qm9tut, 'best_inference_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the test dataloader from the QM( data to obtain a batch of molecules and apply the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dictionary: {'energy_U0': tensor([ 9333.5307, 11332.8081, 10844.4088, 11210.0076, 11836.1201,  9172.6574,\n",
      "        10287.7766, 11334.1123, 11332.9134, 10824.0110, 11785.6308, 11212.4274,\n",
      "        10235.3932, 12243.2745, 12345.7868, 12345.3807, 11796.5354, 12224.1717,\n",
      "        11371.3230, 10751.6529, 10649.7008, 11249.6271, 11798.1534, 12791.7377,\n",
      "        11779.5039, 10358.9509, 10772.0780, 11351.8941, 11231.2139, 10783.5423,\n",
      "         9799.1497, 12242.0594, 11231.4712, 11321.1248, 11372.1191, 11818.0016,\n",
      "        11675.4923, 10203.5050, 10379.0468, 11230.4986,  8132.8889, 11779.0077,\n",
      "        11637.8801, 11372.0278,  9778.6330, 11211.0127, 11230.3103, 12243.3340,\n",
      "        10359.2251,  9771.6275, 12405.5860, 10785.0830, 11372.3786, 12345.5302,\n",
      "        11321.4495, 12790.7953, 11248.1417, 12261.9512, 12389.4074,  9274.6194,\n",
      "        12365.6048, 10358.9336,  9333.0098, 12706.1660, 11778.6273, 11740.6169,\n",
      "        11216.4509, 10378.7661, 10784.3455,  9346.2143, 10784.9676, 11371.9886,\n",
      "        11211.0822,  9770.9341,  9353.7542, 11352.5353, 10764.4645, 10287.6649,\n",
      "        12242.1640, 12366.0468, 10358.9495, 10843.9420, 10823.7237, 13340.1548,\n",
      "        10275.5357, 10398.5837, 12083.9080, 10326.5206, 10784.4797, 11352.4572,\n",
      "        11392.8815, 11797.1991, 11817.7323, 10764.9629, 11692.9489, 12771.0325,\n",
      "        12365.8219, 11797.3898, 11676.7166, 12830.8802], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "for batch in qm9data.test_dataloader():\n",
    "    result = best_model(batch)\n",
    "    print(\"Result dictionary:\", result)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is not already in SchNetPack format, a convenient way is to use ASE atoms with the\n",
    "provided `AtomsConverter` and (optionally) the `SpkCalculator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "converter = spk.interfaces.AtomsConverter(neighbor_list=trn.ASENeighborList(cutoff=5.), dtype=torch.float32)\n",
    "calculator = spk.interfaces.SpkCalculator(model=best_model, converter=converter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "numbers = np.array([6, 1, 1, 1, 1])\n",
    "positions = np.array([[-0.0126981359, 1.0858041578, 0.0080009958],\n",
    "                      [0.002150416, -0.0060313176, 0.0019761204],\n",
    "                      [1.0117308433, 1.4637511618, 0.0002765748],\n",
    "                      [-0.540815069, 1.4475266138, -0.8766437152],\n",
    "                      [-0.5238136345, 1.4379326443, 0.9063972942]])\n",
    "atoms = Atoms(numbers=numbers, positions=positions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['_n_atoms', '_atomic_numbers', '_positions', '_cell', '_pbc', '_idx_i_local', '_idx_j_local', '_Rij', '_idx_m', '_idx_j', '_idx_i']\n",
      "Prediction: tensor([1062.9546], dtype=torch.float64, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = converter(atoms)\n",
    "\n",
    "print('Keys:', list(inputs.keys()))\n",
    "\n",
    "pred = best_model(inputs)\n",
    "\n",
    "print('Prediction:', pred[QM9.U0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can use the `SpkCalculator` as an interface to ASE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1062.9545620828867\n"
     ]
    }
   ],
   "source": [
    "calculator = spk.interfaces.SpkCalculator(model=best_model, converter=converter, energy=QM9.U0, energy_units=\"eV\")\n",
    "atoms.set_calculator(calculator)\n",
    "print('Prediction:', atoms.get_total_energy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The calculator automatically converts the prediction of the given unit to internal ASE units, which is `eV`\n",
    "for the energy.\n",
    "Using the calculator interface makes more sense if you have trained SchNet for a potential energy surface.\n",
    "In the next tutorial, we will show how to learn potential energy surfaces and forces field as well as performing\n",
    "molecular dynamics simulations with SchNetPack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-spkdev-py",
   "language": "python",
   "display_name": "Python [conda env:spkdev] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}