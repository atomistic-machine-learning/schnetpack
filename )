[1mdiff --git a/src/schnetpack/interfaces/batchwise_optimization.py b/src/schnetpack/interfaces/batchwise_optimization.py[m
[1mindex 1bc8842..9a8c28c 100644[m
[1m--- a/src/schnetpack/interfaces/batchwise_optimization.py[m
[1m+++ b/src/schnetpack/interfaces/batchwise_optimization.py[m
[36m@@ -19,6 +19,9 @@[m [mimport torch[m
 from torch import nn[m
 from schnetpack.units import convert_units[m
 from schnetpack.interfaces.ase_interface import AtomsConverter[m
[32m+[m[32mfrom schnetpack import properties[m
[32m+[m[32mfrom schnetpack.data.loader import _atoms_collate_fn[m
[32m+[m[32mfrom schnetpack.transform import CastTo32[m
 [m
 [m
 __all__ = ["ASEBatchwiseLBFGS", "BatchwiseCalculator", "BatchwiseEnsembleCalculator", "NNEnsemble"][m
[36m@@ -149,6 +152,16 @@[m [mclass BatchwiseCalculator:[m
 [m
         self._initialize_model(model)[m
 [m
[32m+[m[32m        # debugging: log forward pass time and nbh list calc. time[m
[32m+[m[32m        self.total_fwd_time = 0.[m
[32m+[m[32m        self.n_fwd_iterations = 0[m
[32m+[m
[32m+[m[32m        self.previous_positions = None[m
[32m+[m[32m        self.previous_cell = None[m
[32m+[m[32m        self.previous_pbc = None[m
[32m+[m
[32m+[m[32m        self.cutoff_skin = 0.3[m
[32m+[m
     def _load_model(self, model: str) -> nn.Module:[m
         return torch.load(model, map_location="cpu").to(torch.float64)[m
 [m
[36m@@ -158,48 +171,64 @@[m [mclass BatchwiseCalculator:[m
         self.model = model.eval()[m
         self.model.to(device=self.device, dtype=self.dtype)[m
 [m
[31m-    def _requires_calculation(self, property_keys: List[str], atoms: List[ase.Atoms]):[m
[32m+[m[32m    def _requires_calculation(self, property_keys: List[str], inputs):[m
         if self.results is None:[m
             return True[m
         for name in property_keys:[m
             if name not in self.results:[m
                 return True[m
[31m-        if len(self.atoms) != len(atoms):[m
[32m+[m[32m        if self.previous_positions is None or self.previous_cell is None or self.previous_pbc is None:[m
[32m+[m[32m            return True[m
[32m+[m[32m        if not torch.equal(inputs["_positions"], self.previous_positions):[m
[32m+[m[32m            return True[m
[32m+[m[32m        if not torch.equal(inputs["_cell"], self.previous_cell):[m
[32m+[m[32m            return True[m
[32m+[m[32m        if not torch.equal(inputs["_pbc"], self.previous_pbc):[m
             return True[m
[31m-        for atom, atom_ref in zip(atoms, self.atoms):[m
[31m-            if atom != atom_ref:[m
[31m-                return True[m
 [m
[31m-    def get_forces(self, atoms: List[ase.Atoms], fixed_atoms_mask: Optional[List[int]] = None) -> np.array:[m
[32m+[m[32m    def get_forces(self, inputs, fixed_atoms_mask: Optional[List[int]] = None) -> np.array:[m
         """[m
         atoms:[m
 [m
         fixed_atoms_mask:[m
             list of indices corresponding to atoms with positions fixed in space.[m
         """[m
[31m-        if self._requires_calculation(property_keys=[self.energy_key, self.force_key], atoms=atoms):[m
[31m-            self.calculate(atoms)[m
[32m+[m[32m        if self._requires_calculation(property_keys=[self.energy_key, self.force_key], inputs=inputs):[m
[32m+[m[32m            self.calculate(inputs)[m
         f = self.results[self.force_key][m
         if fixed_atoms_mask is not None:[m
[31m-            f[fixed_atoms_mask] = 0.0[m
[32m+[m[32m            f = f[fixed_atoms_mask][m
         return f[m
 [m
[31m-    def get_potential_energy(self, atoms: List[ase.Atoms]) -> float:[m
[31m-        if self._requires_calculation(property_keys=[self.energy_key], atoms=atoms):[m
[31m-            self.calculate(atoms)[m
[32m+[m[32m    def get_potential_energy(self, inputs) -> float:[m
[32m+[m[32m        if self._requires_calculation(property_keys=[self.energy_key], inputs=inputs):[m
[32m+[m[32m            self.calculate(inputs)[m
         return self.results[self.energy_key][m
 [m
[31m-    def calculate(self, atoms: List[ase.Atoms]) -> None:[m
[32m+[m[32m    def calculate(self, inputs) -> None:[m
[32m+[m
[32m+[m[32m        inputs = deepcopy(inputs)[m
         property_keys = list(self.property_units.keys())[m
[31m-        inputs = self.atoms_converter(atoms)[m
[32m+[m
[32m+[m[32m        inputs = self.atoms_converter.update_inputs(inputs)[m
[32m+[m
[32m+[m[32m        self.previous_positions = inputs[properties.R].clone()[m
[32m+[m[32m        self.previous_cell = inputs[properties.cell].clone()[m
[32m+[m[32m        self.previous_pbc = inputs[properties.pbc].clone()[m
[32m+[m
[32m+[m[32m        # track fwd. pass time and count iterations[m
[32m+[m[32m        self.n_fwd_iterations += 1[m
[32m+[m[32m        ts = time.time()[m
         model_results = self.model(inputs)[m
[32m+[m[32m        te = time.time()[m
[32m+[m[32m        self.total_fwd_time += te - ts[m
 [m
         results = {}[m
         # store model results in calculator[m
         for prop in property_keys:[m
             if prop in model_results:[m
                 results[prop] = ([m
[31m-                    model_results[prop].detach().cpu().numpy()[m
[32m+[m[32m                    model_results[prop].detach()[m
                     * self.property_units[prop][m
                 )[m
             else:[m
[36m@@ -210,7 +239,6 @@[m [mclass BatchwiseCalculator:[m
                 )[m
 [m
         self.results = results[m
[31m-        self.atoms = atoms.copy()[m
 [m
 [m
 class BatchwiseEnsembleCalculator(BatchwiseCalculator):[m
[36m@@ -342,13 +370,13 @@[m [mclass BatchwiseDynamics(Dynamics):[m
     def __init__([m
         self,[m
         calculator: BatchwiseCalculator,[m
[31m-        atoms: List[Atoms],[m
[32m+[m[32m        inputs: Dict[str, torch.Tensor],[m
         logfile: str,[m
         trajectory: Optional[str],[m
         append_trajectory: bool = False,[m
         master: Optional[bool] = None,[m
         log_every_step: bool = False,[m
[31m-        fixed_atoms_mask: Optional[List[int]]=None,[m
[32m+[m[32m        fixed_atoms_mask: Optional[List[int]] = None,[m
     ):[m
         """Structure dynamics object.[m
 [m
[36m@@ -387,7 +415,7 @@[m [mclass BatchwiseDynamics(Dynamics):[m
             list of indices corresponding to atoms with positions fixed in space.[m
         """[m
         super().__init__([m
[31m-            atoms=atoms,[m
[32m+[m[32m            atoms=None,[m
             logfile=logfile,[m
             trajectory=trajectory,[m
             append_trajectory=append_trajectory,[m
[36m@@ -397,18 +425,39 @@[m [mclass BatchwiseDynamics(Dynamics):[m
         self.calculator = calculator[m
         self.trajectory = trajectory[m
         self.log_every_step = log_every_step[m
[31m-        self.fixed_atoms_mask = fixed_atoms_mask[m
[31m-        self.n_configs = len(self.atoms)[m
[31m-        self.n_atoms = len(self.atoms[0])[m
[32m+[m[32m        self.fixed_atoms_mask = ~torch.tensor(fixed_atoms_mask)[m
[32m+[m
[32m+[m[32m        self.inputs = inputs[m
[32m+[m[32m        self.n_configs = inputs["_n_atoms"].shape[0][m
[32m+[m
[32m+[m[32m    def _build_ase_atoms(self):[m
[32m+[m[32m        ts = time.time()[m
[32m+[m[32m        ats = [][m
[32m+[m[32m        n_configs = self.inputs["_n_atoms"].shape[0][m
[32m+[m[32m        cells = torch.diagonal(self.inputs["_cell"], offset=0, dim1=-2, dim2=-1)[m
[32m+[m[32m        for config_idx in range(n_configs):[m
[32m+[m[32m            pos = self.inputs["_positions"][self.inputs["_idx_m"] == config_idx].cpu().numpy()[m
[32m+[m[32m            at_nums = self.inputs["_atomic_numbers"][self.inputs["_idx_m"] == config_idx].cpu().numpy()[m
[32m+[m[32m            at = Atoms([m
[32m+[m[32m                positions=pos,[m
[32m+[m[32m                numbers=at_nums,[m
[32m+[m[32m            )[m
[32m+[m[32m            at.pbc = self.inputs["_pbc"][config_idx].cpu().numpy()[m
[32m+[m[32m            at.set_cell(cells[config_idx].cpu().numpy())[m
[32m+[m[32m            ats.append(at)[m
[32m+[m[32m        self.atoms = ats[m
[32m+[m[32m        te = time.time()[m
[32m+[m[32m        self.ase_time += te - ts[m
 [m
     def irun(self):[m
         # compute initial structure and log the first step[m
[31m-        self.calculator.get_forces(self.atoms, fixed_atoms_mask=self.fixed_atoms_mask)[m
[32m+[m[32m        self.calculator.get_forces(self.inputs, fixed_atoms_mask=self.fixed_atoms_mask)[m
 [m
         # yield the first time to inspect before logging[m
         yield False[m
 [m
         if self.nsteps == 0:[m
[32m+[m[32m            self._build_ase_atoms()[m
             self.log()[m
             pass[m
 [m
[36m@@ -425,9 +474,11 @@[m [mclass BatchwiseDynamics(Dynamics):[m
 [m
             # log the step[m
             if self.log_every_step:[m
[32m+[m[32m                self._build_ase_atoms()[m
                 self.log()[m
 [m
         # log last step[m
[32m+[m[32m        self._build_ase_atoms()[m
         self.log()[m
 [m
         # finally check if algorithm was converged[m
[36m@@ -454,7 +505,7 @@[m [mclass BatchwiseOptimizer(BatchwiseDynamics):[m
     def __init__([m
         self,[m
         calculator: BatchwiseCalculator,[m
[31m-        atoms: List[Atoms],[m
[32m+[m[32m        inputs: Dict[str, torch.Tensor],[m
         restart: Optional[bool] = None,[m
         logfile: Optional[str] = None,[m
         trajectory: Optional[str] = None,[m
[36m@@ -502,7 +553,7 @@[m [mclass BatchwiseOptimizer(BatchwiseDynamics):[m
         BatchwiseDynamics.__init__([m
             self,[m
             calculator=calculator,[m
[31m-            atoms=atoms,[m
[32m+[m[32m            inputs=inputs,[m
             logfile=logfile,[m
             trajectory=trajectory,[m
             master=master,[m
[36m@@ -550,15 +601,14 @@[m [mclass BatchwiseOptimizer(BatchwiseDynamics):[m
         """Did the optimization converge?"""[m
         if forces is None:[m
             forces = self.calculator.get_forces([m
[31m-                self.atoms, fixed_atoms_mask=self.fixed_atoms_mask[m
[32m+[m[32m                self.inputs, fixed_atoms_mask=self.fixed_atoms_mask[m
             )[m
[31m-        # todo: maybe np.linalg.norm?[m
         return (forces**2).sum(axis=1).max() < self.fmax**2[m
 [m
     def log(self, forces: Optional[np.array] = None) -> None:[m
         if forces is None:[m
             forces = self.calculator.get_forces([m
[31m-                self.atoms, fixed_atoms_mask=self.fixed_atoms_mask[m
[32m+[m[32m                self.inputs, fixed_atoms_mask=self.fixed_atoms_mask[m
             )[m
         fmax = sqrt((forces**2).sum(axis=1).max())[m
         T = time.localtime()[m
[36m@@ -586,7 +636,8 @@[m [mclass BatchwiseOptimizer(BatchwiseDynamics):[m
                 )[m
 [m
     def get_relaxation_results(self) -> Tuple[Atoms, Dict]:[m
[31m-        self.calculator.get_forces(self.atoms)[m
[32m+[m[32m        self._build_ase_atoms()[m
[32m+[m[32m        self.calculator.get_forces(self.inputs)[m
         return self.atoms, self.calculator.results[m
 [m
     def dump(self, data):[m
[36m@@ -612,7 +663,7 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
     def __init__([m
         self,[m
         calculator: BatchwiseCalculator,[m
[31m-        atoms: List[Atoms],[m
[32m+[m[32m        inputs: Dict[str, torch.Tensor],[m
         restart: Optional[bool] = None,[m
         logfile: str = "-",[m
         trajectory: Optional[str] = None,[m
[36m@@ -625,6 +676,7 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
         log_every_step: bool = False,[m
         fixed_atoms_mask: Optional[List[int]] = None,[m
         verbose: bool = False,[m
[32m+[m[32m        device: torch.device = torch.device("cuda"),[m
     ):[m
 [m
         """Parameters:[m
[36m@@ -684,7 +736,7 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
         BatchwiseOptimizer.__init__([m
             self,[m
             calculator=calculator,[m
[31m-            atoms=atoms,[m
[32m+[m[32m            inputs=inputs,[m
             restart=restart,[m
             logfile=logfile,[m
             trajectory=trajectory,[m
[36m@@ -720,6 +772,12 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
         if use_line_search:[m
             raise NotImplementedError("Lines search has not been implemented yet")[m
 [m
[32m+[m[32m        # debugging: log forward pass time and nbh list calc. time[m
[32m+[m[32m        self.total_opt_time = 0.[m
[32m+[m[32m        self.ase_time = 0.[m
[32m+[m
[32m+[m[32m        self.device = device[m
[32m+[m
     def initialize(self) -> None:[m
         """Initialize everything so no checks have to be done in step"""[m
         self.iteration = 0[m
[36m@@ -757,23 +815,18 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
 [m
         if f is None:[m
             f = self.calculator.get_forces([m
[31m-                self.atoms, fixed_atoms_mask=self.fixed_atoms_mask[m
[31m-            )[m
[32m+[m[32m                self.inputs, fixed_atoms_mask=self.fixed_atoms_mask[m
[32m+[m[32m            ).to(self.device)[m
[32m+[m
[32m+[m[32m        ts = time.time()[m
 [m
         # check if updates for respective structures are required[m
         q_euclidean = -f.reshape(self.n_configs, -1, 3)[m
[31m-        squared_max_forces = (q_euclidean**2).sum(axis=-1).max(axis=-1)[m
[32m+[m[32m        squared_max_forces = (q_euclidean**2).sum(axis=-1).max(axis=-1)[0][m
         configs_mask = squared_max_forces < self.fmax**2[m
[31m-        mask = ([m
[31m-            configs_mask[:, None][m
[31m-            .repeat(q_euclidean.shape[1], 0)[m
[31m-            .repeat(q_euclidean.shape[2], 1)[m
[31m-        )[m
[31m-        r = np.zeros((self.n_atoms * self.n_configs, 3), dtype=np.float64)[m
[31m-        for config_idx, at in enumerate(self.atoms):[m
[31m-            first_idx = config_idx * self.n_atoms[m
[31m-            last_idx = config_idx * self.n_atoms + self.n_atoms[m
[31m-            r[first_idx:last_idx] = at.get_positions()[m
[32m+[m[32m        mask = configs_mask[:, None, None].repeat(1, q_euclidean.shape[1], q_euclidean.shape[2]).view(-1, 3)[m
[32m+[m
[32m+[m[32m        r = self.inputs["_positions"][self.fixed_atoms_mask].to(torch.float64).to(self.device)[m
 [m
         self.update(r, f, self.r0, self.f0)[m
 [m
[36m@@ -783,30 +836,30 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
         H0 = self.H0[m
 [m
         loopmax = np.min([self.memory, self.iteration])[m
[31m-        a = np.empty([m
[32m+[m[32m        a = torch.empty([m
             ([m
                 loopmax,[m
                 self.n_configs,[m
                 1,[m
                 1,[m
             ),[m
[31m-            dtype=np.float64,[m
[31m-        )[m
[32m+[m[32m            dtype=torch.float64,[m
[32m+[m[32m        ).to(self.device)[m
 [m
         # ## The algorithm itself:[m
         q = -f.reshape(self.n_configs, 1, -1)[m
         for i in range(loopmax - 1, -1, -1):[m
[31m-            a[i] = rho[i] * np.matmul(s[i], np.transpose(q, axes=(0, 2, 1)))[m
[32m+[m[32m            a[i] = rho[i] * torch.matmul(s[i], torch.transpose(q, 2, 1))[m
             q -= a[i] * y[i][m
 [m
         z = H0 * q[m
 [m
         for i in range(loopmax):[m
[31m-            b = rho[i] * np.matmul(y[i], np.transpose(z, axes=(0, 2, 1)))[m
[32m+[m[32m            b = rho[i] * torch.matmul(y[i], torch.transpose(z, 2, 1))[m
             z += s[i] * (a[i] - b)[m
 [m
         p = -z.reshape((-1, 3))[m
[31m-        self.p = np.where(mask, np.zeros_like(p), p)[m
[32m+[m[32m        self.p = torch.where(mask, torch.zeros_like(p), p)[m
         # ##[m
 [m
         g = -f[m
[36m@@ -820,21 +873,7 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
             dr = self.determine_step(self.p) * self.damping[m
 [m
         # update positions[m
[31m-        pos_updated = r + dr[m
[31m-[m
[31m-        # create new list of ase Atoms objects with updated positions[m
[31m-        ats = [][m
[31m-        for config_idx, at in enumerate(self.atoms):[m
[31m-            first_idx = config_idx * self.n_atoms[m
[31m-            last_idx = config_idx * self.n_atoms + self.n_atoms[m
[31m-            at = Atoms([m
[31m-                positions=pos_updated[first_idx:last_idx],[m
[31m-                numbers=self.atoms[config_idx].get_atomic_numbers(),[m
[31m-            )[m
[31m-            at.pbc = self.atoms[config_idx].pbc[m
[31m-            at.cell = self.atoms[config_idx].cell[m
[31m-            ats.append(at)[m
[31m-        self.atoms = ats[m
[32m+[m[32m        self.inputs["_positions"][self.fixed_atoms_mask] += dr.to(self.calculator.device).to(torch.float32)[m
 [m
         self.iteration += 1[m
         self.r0 = r[m
[36m@@ -852,6 +891,9 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
             )[m
         )[m
 [m
[32m+[m[32m        te = time.time()[m
[32m+[m[32m        self.total_opt_time += te - ts[m
[32m+[m
     def determine_step(self, dr: np.array) -> np.array:[m
         """Determine step to take according to maxstep[m
 [m
[36m@@ -860,12 +902,13 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
         """[m
         steplengths = (dr**2).sum(-1) ** 0.5[m
         # check if any step in entire batch is greater than maxstep[m
[31m-        if np.max(steplengths) >= self.maxstep:[m
[32m+[m[32m        if torch.max(steplengths) >= self.maxstep:[m
             # rescale steps for each config separately[m
             for config_idx in range(self.n_configs):[m
[31m-                first_idx = config_idx * self.n_atoms[m
[31m-                last_idx = config_idx * self.n_atoms + self.n_atoms[m
[31m-                longest_step = np.max(steplengths[first_idx:last_idx])[m
[32m+[m[32m                # TODO: make this more general[m
[32m+[m[32m                first_idx = config_idx * 42[m
[32m+[m[32m                last_idx = config_idx * 42 + 42[m
[32m+[m[32m                longest_step = torch.max(steplengths[first_idx:last_idx])[m
                 if longest_step >= self.maxstep:[m
                     if self.verbose:[m
                         print("normalized integration step")[m
[36m@@ -879,18 +922,16 @@[m [mclass ASEBatchwiseLBFGS(BatchwiseOptimizer):[m
         This function is mostly here to allow for replay_trajectory.[m
         """[m
         if self.iteration > 0:[m
[31m-            s0 = r.reshape(self.n_configs, 1, -1) - r0.reshape(self.n_configs, 1, -1)[m
[32m+[m
[32m+[m[32m            s0 = (r - r0).view(self.n_configs, 1, -1)[m
             self.s.append(s0)[m
 [m
             # We use the gradient which is minus the force![m
[31m-            y0 = f0.reshape(self.n_configs, 1, -1) - f.reshape(self.n_configs, 1, -1)[m
[32m+[m[32m            y0 = (f0 - f).view(self.n_configs, 1, -1)[m
             self.y.append(y0)[m
 [m
[31m-            rho0 = np.ones((self.n_configs, 1, 1), dtype=np.float64)[m
[31m-            for config_idx in range(self.n_configs):[m
[31m-                ys0 = np.dot(y0[config_idx, 0], s0[config_idx, 0])[m
[31m-                if ys0 > 1e-8:[m
[31m-                    rho0[config_idx, 0, 0] = 1.0 / ys0[m
[32m+[m[32m            ys0 = torch.matmul(y0, s0.transpose(1, 2))[m
[32m+[m[32m            rho0 = torch.where(ys0 > 1e-8, 1.0 / ys0, 0.0)[m
             self.rho.append(rho0)[m
 [m
         if self.iteration > self.memory:[m
